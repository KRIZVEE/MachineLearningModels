{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Welcome to Part 6 - Dimensionality Reduction!\n",
    "\n",
    "\n",
    "\n",
    "Remember in Part 3 - Classification, we worked with datasets composed of only two independent variables. We did for two reasons:\n",
    "\n",
    "Because we needed two dimensions to visualize better how Machine Learning models worked (by plotting the prediction regions and the prediction boundary for each model).\n",
    "Because whatever is the original number of our independent variables, we can often end up with two independent variables by applying an appropriate Dimensionality Reduction technique.\n",
    "\n",
    "\n",
    "There are two types of Dimensionality Reduction techniques:\n",
    "\n",
    "Feature Selection\n",
    "Feature Extraction\n",
    "\n",
    "\n",
    "Feature Selection techniques are Backward Elimination, Forward Selection, Bidirectional Elimination, Score Comparison and more. We covered these techniques in Part 2 - Regression.\n",
    "\n",
    "In this part we will cover the following Feature Extraction techniques:\n",
    "\n",
    "Principal Component Analysis (PCA)\n",
    "Linear Discriminant Analysis (LDA)\n",
    "Kernel PCA\n",
    "Quadratic Discriminant Analysis (QDA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
